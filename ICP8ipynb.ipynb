{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv7+TU7oNEczrT5u2X88or",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vezr96/Class-Programming-1-Vance/blob/main/ICP8ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3WlmeREZJ4g",
        "outputId": "550e4c71-3a40-42df-f9f5-623af9c804d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "McOjB4HscFsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Wtgy87G5YIrj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ecadc9-5be8-4ff0-a784-f16a433e6ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD Elements:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
            "Number of Partitions:  2\n",
            "First Element:  1\n",
            "Even Elements:  [2, 4, 6, 8, 10, 12, 14]\n",
            "Squared Elements:  [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225]\n",
            "Sum of Elements:  120\n",
            "Union of RDD1 and RDD2:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Cartesian RDD:  [(1, 6), (1, 7), (2, 6), (2, 7), (1, 8), (1, 9), (2, 8), (2, 9), (1, 10), (2, 10), (3, 6), (3, 7), (4, 6), (4, 7), (5, 6), (5, 7), (3, 8), (3, 9), (4, 8), (4, 9), (3, 10), (4, 10), (5, 8), (5, 9), (5, 10)]\n",
            "Unique Values:  [('b', 1), ('c', 1), ('a', 1)]\n",
            "First 5 Lines:  [1, 2, 3, 4, 5]\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|  Tim| 30|\n",
            "|  Tom| 25|\n",
            "|Terry| 35|\n",
            "+-----+---+\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|  Tim| 30|\n",
            "|  Tom| 25|\n",
            "|Terry| 35|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql import types\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"RDD Operations\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "RDD = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9 ,10, 11, 12, 13, 14, 15])\n",
        "print(\"RDD Elements: \", RDD.collect())\n",
        "print(\"Number of Partitions: \", RDD.getNumPartitions())\n",
        "\n",
        "firstElement = RDD.first()\n",
        "print(\"First Element: \", firstElement)\n",
        "\n",
        "Even_RDD = RDD.filter(lambda x: x % 2 == 0)\n",
        "print(\"Even Elements: \", Even_RDD.collect())\n",
        "\n",
        "Squared_RDD = RDD.map(lambda x: x ** 2)\n",
        "print(\"Squared Elements: \", Squared_RDD.collect())\n",
        "\n",
        "sum_of_elements = RDD.reduce(lambda x, y: x + y)\n",
        "print(\"Sum of Elements: \", sum_of_elements)\n",
        "\n",
        "#RDD.saveAsTextFile(\"/content/drive/MyDrive/RDD_data.txt\")\n",
        "\n",
        "RDD1 = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
        "RDD2 = spark.sparkContext.parallelize([6, 7, 8, 9, 10])\n",
        "union_RDD = RDD1.union(RDD2)\n",
        "print(\"Union of RDD1 and RDD2: \", union_RDD.collect())\n",
        "\n",
        "Cartesian_RDD = RDD1.cartesian(RDD2)\n",
        "print(\"Cartesian RDD: \", Cartesian_RDD.collect())\n",
        "\n",
        "Dictionary_RDD = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2), (\"c\", 3)])\n",
        "\n",
        "Unique_value = Dictionary_RDD.groupByKey().mapValues(len)\n",
        "print(\"Unique Values: \", Unique_value.collect())\n",
        "\n",
        "file_path1 = \"/content/drive/MyDrive/data/Num1.txt\"\n",
        "file_path2 = \"/content/drive/MyDrive/data/Num2.txt\"\n",
        "conf = SparkConf().setAppName(\"CombineTextFiles\")\n",
        "RDD1 = spark.sparkContext.textFile(file_path1)\n",
        "RDD2 = spark.sparkContext.textFile(file_path2)\n",
        "Combined_RDD = RDD1.union(RDD2)\n",
        "print(\"Combined RDD: \", Combined_RDD.collect())\n",
        "\n",
        "RDD_5lines = RDD.take(5)\n",
        "print(\"First 5 Lines: \", RDD_5lines)\n",
        "\n",
        "data = [['Tim', 22], ['Tom', 55], ['Terry', 45]]\n",
        "columns = [\"name\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "spark = SparkSession.builder.appName(\"DatasetExample\").getOrCreate()\n",
        "schema = types.StructType([\n",
        "    types.StructField(\"name\", types.StringType(), True),\n",
        "    types.StructField(\"age\", types.IntegerType(), True)\n",
        "])\n",
        "RDD_Schema = spark.createDataFrame(data, schema)\n",
        "RDD_Schema.show()\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RDD_D\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RDD, DataFrame, Dataset\").getOrCreate()\n",
        "\n",
        "data = [['Tim', 22], ['Tom', 55], ['Terry', 45]]\n",
        "\n",
        "RDD = spark.sparkContext.parallelize(data)\n",
        "for element in RDD.collect():\n",
        "  print(element)\n",
        "Filtered_RDD = RDD.filter(lambda x: x[1] > 30)\n",
        "print(Filtered_RDD.collect())\n",
        "\n",
        "DF = RDD.toDF()\n",
        "DF.show()\n",
        "Filtered_DF = DF.filter(DF._2 > 30)\n",
        "Filtered_DF.show\n",
        "\n",
        "Schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "DS = spark.createDataFrame(data, Schema)\n",
        "DS.show()\n",
        "Filtered_DS = DS.filter(DS.age > 30)\n",
        "Filtered_DS.show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQUXbuYlXDJ-",
        "outputId": "23c81d43-da62-426e-b1e2-33698b859c8a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tim', 22]\n",
            "['Tom', 55]\n",
            "['Terry', 45]\n",
            "[['Tom', 55], ['Terry', 45]]\n",
            "+-----+---+\n",
            "|   _1| _2|\n",
            "+-----+---+\n",
            "|  Tim| 22|\n",
            "|  Tom| 55|\n",
            "|Terry| 45|\n",
            "+-----+---+\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|  Tim| 22|\n",
            "|  Tom| 55|\n",
            "|Terry| 45|\n",
            "+-----+---+\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|  Tom| 55|\n",
            "|Terry| 45|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD: Represents a distrobution of elementsacross a cluster and can be made from a list, file, or other sources of data.\n",
        "Dataframe: A distrobution of data that is organized in named colums, provides higher level of API than ADD, and made for performance.\n",
        "Dataset: Usees both qualites of RDD and Dataframes, can be used for functional programming paradigms."
      ],
      "metadata": {
        "id": "KvTeSqWPZzfc"
      }
    }
  ]
}